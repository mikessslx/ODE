\section{Lecture 15 - 12.02}

\begin{theorem}
{\color{blue}\begin{enumerate}[a)]
\item A matrix \(Q\) is a fundamental matrix of solutions to \(\frac{dy}{dt} = Ay\) if and only if \(\frac{dQ}{dt} = AQ\) and \(\det(Q)(t_0) \neq 0\) for some \(t_0 \in \mathbb{R}\).
\item Let \(Q\) and \(R\) be two fundamental matrices for \(\frac{dy}{dt} = Ay\). Then there exists a constant matrix \(C\) s.t. \(R(t) = Q(t)C\) for every \(t \in \mathbb{R}\).
\item The matrix exponential \(e^{At}\) is a fundamental matrix solution to \(\frac{dy}{dt} = Ay\). Also, if \(Q\) is a fundamental matrix for the same system, then \(e^{At} = Q(t)(Q(0))^{-1}\) for every \(t \in \mathbb{R}\).
\end{enumerate}}
\end{theorem}

\begin{remark}
\sout{Proof is commented out!}
\end{remark}

% \begin{proof}
% \begin{enumerate}[a)]
% \item
% We've already proved that if \(Q\) is a fundamental matrix for \(\frac{dy}{dt} = Ay\), then \(\frac{dQ}{dt} = AQ\) and \(\det(Q)(t) \neq 0\) for every \(t\) (see previous lectures).

% Assume that \(Q\) satisfies \(\frac{dQ}{dt} = AQ\) and \(\det(Q)(t_0) \neq 0\) for some \(t_0 \in \mathbb{R}\). Let \(y^{(j)}\) be the \(j\)-th column of \(Q\), that is
% \[Q = \underbrace{\left( y^{(1)} \ y^{(2)} \ \cdots \ y^{(n)} \right)}_{\text{column vectors}}\]

% Notice that \(\frac{dQ}{dt} = (\frac{dy^{(1)}}{dt} \ \frac{dy^{(2)}}{dt} \ \cdots \ \frac{dy^{(n)}}{dt})\) and \(AQ = (Ay^{(1)} \ Ay^{(2)} \ \cdots \ Ay^{(n)})\).

% Then, \(\frac{dy^{(j)}}{dt} = Ay^{(j)}\), \(j=1, \cdots, n\). The columns of \(Q\) are solutions to \(\frac{dy}{dt} = Ay\). Also, the condition \(\det(Q)(t_0) \neq 0\) implies that \(\{y^{(1)}(t_0), y^{(2)}(t_0), \cdots, y^{(n)}(t_0)\}\) is linearly independent. Then \(\{y^{(1)}, \cdots, y^{(n)}\}\) is a basis of solutions and so \(Q\) is a fundamental matrix for \(\frac{dy}{dt} = Ay\).

% \item
% Let \(R\) and \(Q\) be two fundamental matrix solutions, \(Q = (y^{(1)} \cdots y^{(n)})\) and \(R = (z^{(1)} \cdots z^{(n)})\).

% Notice that there exist \(c_{1j}, c_{2j}, \cdots, c_{nj} \in \mathbb{R}\) s.t.
% \[z^{(j)} = c_{1j}y^{(1)} + \cdots + c_{nj}y^{(n)}, \ j=1, \cdots, n\]

% Now consider \(C = (c_{ij})_{i,j=1,\cdots,n}\), we get \(R(t) = Q(t)C\).

% \item
% We've already proved that \(\frac{d e^{At}}{dt} = A e^{At}\), for every \(t\).

% Also, \(e^{At}|_{t=0} = (I + At + \frac{A^2 t^2}{2} + \cdots)|_{t=0} = I\). Then \(\det(e^{At}|_{t=0}) \neq 0\).

% By part (a), it follows that \(e^{At}\) is a fundamental matrix solution for \(\frac{dy}{dt} = A y\).

% Assume that \(Q\) is a fundamental matrix solution for \(\frac{dy}{dt} = Ay\).

% By part (b), we know that there exists \(C \in \mathbb{R}^{n \times n}\) s.t. \(e^{At} = Q(t)C\) for every \(t\).

% In particular, if \(t=0\) then \(I = Q(0)C\). As \(Q\) is a fundamental matrix of solutions, \(\det(Q(0)) \neq 0\), then \(C = (Q(0))^{-1}\). Therefore, \(e^{At} = Q(t)(Q(0))^{-1}\) for every \(t\).
% \end{enumerate}
% \end{proof}

Now consider the non-homogeneous problem
\[\frac{dy}{dt} = Ay + b(t),\]
where \(b\) is continuous in \(\mathbb{R}\).

Let \(Q\) be a fundamental matrix solution to \(\frac{dy}{dt} = Ay\). We know that \(y(t) = Q(t)C(t)\) is a solution to \(\frac{dy}{dt} = Ay + b(t)\) if and only if \(\frac{dC(t)}{dt} = (Q(t))^{-1}b(t)\) for every \(t\).

Integration yields
\begin{align*}
C(t) &= C(t_0) + \int_{t_0}^t (Q(s))^{-1}b(s) ds, \text{ where } t_0 \in \mathbb{R} \\
&= (Q(t_0))^{-1}y(t_0) + \int_{t_0}^t (Q(s))^{-1}b(s) ds
\end{align*}

In particular, if \(Q(t) = e^{At}\), then
{\color{blue}\[y(t) = e^{At} \left( e^{-At_0} y(t_0) + \int_{t_0}^t e^{-As} b(s) ds \right)\]}

\begin{note}
The inverse of \(e^{At}\) is \(e^{-At}\) for every \(t\).
\end{note}

That is,
{\color{blue}\[y(t) = e^{A(t-t_0)} y(t_0) + \int_{t_0}^t e^{A(t-s)} b(s) ds, \ \forall t \in \mathbb{R}\]}

\sout{(This can be generalized to}
\[\frac{dy}{dt} = Ay + b(t), \ A: V \to W\]

\sout{\(V, W\) are function spaces of (possible) infinite dimension \(\rightsquigarrow\) Cf Semigroup theory for PDE.)}

\begin{recap}
Summary:
\begin{itemize}
\item \(\frac{dy}{dt} = A(t)y + b(t)\): general solution \(\checkmark\)
\item \(\frac{dy}{dt} = Ay + b(t)\): solution method \(\checkmark\)
\item \(\frac{d^n y}{dt^n} + a_{n-1}(t) \frac{d^{n-1}y}{dt^{n-1}} + \cdots + a_1(t) \frac{dy}{dt} + a_0(t) = b(t)\): general solution \(\checkmark\)
\item \(\frac{d^n y}{dt^n} + a_{n-1} \frac{d^{n-1}y}{dt^{n-1}} + \cdots + a_1 \frac{dy}{dt} + a_0 y = b(t)\): solution method \(\checkmark\)
\end{itemize}
\end{recap}

\subsection{\S5. Solution Methods for Linear Second-Order Equations}

The general form of a second-order equation is \(F(t, y, \frac{dy}{dt}, \frac{d^2 y}{dt^2}) = 0\).

In particular, linear eqs can be written as
\[c(t) \frac{d^2 y}{dt^2} + a(t) \frac{dy}{dt} + b(t) y(t) = f(t) \ (c \neq 0)\]

We'll present a solution method for eqs. with const. coeff. and then a solution method for eqs. with non-constant coeff.

\begin{example}
\leavevmode
\begin{enumerate}[a)]
\item
{\color{blue}The equation \(m \frac{d^2 y}{dt^2} = F(t, y, \frac{dy}{dt})\) is the Newton's second law of motion.}
        
{\color{blue}The force \(F\) can be nonlinear.}
        
\item
{\color{blue}The equation \(\frac{d^2 \theta}{dt^2} + \frac{g}{L} \sin(\theta) = 0\) is the pendulum equation.}

\item
{\color{blue}The equation \(t^2 \frac{d^2 y}{dt^2} + \alpha t \frac{dy}{dt} + \beta y = 0, \ \alpha, \beta \in \mathbb{R}\), is the Euler's equation.}

\item
{\color{blue}The eq. \(x^2 \frac{d^2 y}{dx^2} + x \frac{dy}{dx} + (x^2 - \nu^2)y = 0\) is the Bessel's eq.}
\end{enumerate}
\end{example}

Let's first revisit the solution method for second-order eqs. with linear constant coeff:
\[\frac{d^2 y}{dt^2} + a \frac{dy}{dt} + by = 0.\]

The associated first order system is
\[\begin{cases}
\frac{dy_1}{dt} = y_2 \\
\frac{dy_2}{dt} = -by_1 - ay_2
\end{cases} \ \text{for } y_1 = y, \ y_2 = \frac{dy}{dt}\]

The coeff. matrix is \(A = \begin{pmatrix} 0 & 1 \\ -b & -a \end{pmatrix}\). The char polynomial is
\[p(\lambda) = \det(A - \lambda I) = \det \begin{pmatrix} -\lambda & 1 \\ -b & -a-\lambda \end{pmatrix} = \lambda(a+\lambda) + b\]

That is, \(p(\lambda) = \lambda^2 + a\lambda + b\). This polynomial is called the char. pol. of the equation.

\subsubsection{Case 1: Assume that \(p(\lambda)\) has two real distinct roots, \(\lambda_1, \lambda_2\).}

Let \(v_1\) and \(v_2\) be eigenvector for \(\lambda_1\) and \(\lambda_2\), respectively. {\color{blue}The functions
\[y^{(1)}(t) = v_1 e^{\lambda_1 t} \ \text{and} \ y^{(2)}(t) = v_2 e^{\lambda_2 t}, \ t \in \mathbb{R}\]
form a basis of solutions to the system.}
If \(v_1 = \begin{pmatrix} u_1 \\ w_1 \end{pmatrix}\) and \(v_2 = \begin{pmatrix} u_2 \\ w_2 \end{pmatrix}\) then \(u_1 e^{\lambda_1 t}\) and \(u_2 e^{\lambda_2 t}\), \(t \in \mathbb{R}\), are solutions to the eq. Then, we notice that \(y_1(t) = e^{\lambda_1 t}\) and \(y_2(t) = e^{\lambda_2 t}\), \(t \in \mathbb{R}\), form a basis of solutions.

\subsubsection{Case 2: Assume that \(p(\lambda)\) has a pair of complex conjugates roots, \(\lambda, \bar{\lambda}\).}

Let \(\lambda = \alpha + i\beta\), \(\beta \neq 0\).

If \(v = u + iw, \ u, w \in \mathbb{R}^n\), is an eigenvector for \(\lambda\), {\color{blue}then the functions:
\[e^{\alpha t} (u \cos(\beta t) - w \sin(\beta t)) \ \text{and} \ e^{\alpha t} (w \cos(\beta t) + u \sin(\beta t)), \ t \in \mathbb{R}\]
form a basis of solutions to the system.}
